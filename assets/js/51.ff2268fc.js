(window.webpackJsonp=window.webpackJsonp||[]).push([[51],{596:function(t,a,s){"use strict";s.r(a);var r=s(35),i=Object(r.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"概率图模型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#概率图模型"}},[t._v("#")]),t._v(" 概率图模型")]),t._v(" "),s("div",{staticClass:"custom-block warning"},[s("p",{staticClass:"custom-block-title"},[t._v("WARNING")]),t._v(" "),s("p",[t._v("UNDER CONSTRUCTION")])]),t._v(" "),s("h2",{attrs:{id:"林达华综述-2013"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#林达华综述-2013"}},[t._v("#")]),t._v(" 林达华综述 2013")]),t._v(" "),s("p",[s("a",{attrs:{href:"http://www.datakit.cn/blog/2014/11/26/PGM_linDahua_interview.html",target:"_blank",rel:"noopener noreferrer"}},[t._v("http://www.datakit.cn/blog/2014/11/26/PGM_linDahua_interview.html"),s("OutboundLink")],1)]),t._v(" "),s("h3",{attrs:{id:"图模型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#图模型"}},[t._v("#")]),t._v(" 图模型")]),t._v(" "),s("p",[t._v("按照采用什么类型的图来表达变量之间的关系可以分为")]),t._v(" "),s("ol",[s("li",[t._v("贝叶斯网络     → 有向无环图")]),t._v(" "),s("li",[t._v("马尔可夫随机场 → 无向图")])]),t._v(" "),s("p",[t._v("这种分类主要是为了研究和学习的便利，在实际应用中常常结合使用")]),t._v(" "),s("p",[t._v("建模和推断两步")]),t._v(" "),s("h3",{attrs:{id:"建模部分的发展方向"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#建模部分的发展方向"}},[t._v("#")]),t._v(" 建模部分的发展方向")]),t._v(" "),s("p",[t._v("在实际问题中，事先设计合适的模型结构是很难的"),s("br"),t._v("\n→ 结构学习，先只设计模型需要满足的约束，然后在学习的过程中形成实际结构")]),t._v(" "),s("p",[t._v("主要问题就是如何发现变量之间的内部关联，做法没有固定的形式，"),s("br"),t._v("\n例如先选完全图连接所有的变量，然后选择一个子图来描述它们的实际结构，"),s("br"),t._v("\n又或者，你可以引入潜在节点（latent node）来建立变量之间的关联")]),t._v(" "),s("p",[t._v("另一个方向就是非参数化，典型的非参数化模型就是基于狄利克莱过程（Dirichlet Process）的混合模型")]),t._v(" "),s("h3",{attrs:{id:"统计推断"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#统计推断"}},[t._v("#")]),t._v(" 统计推断")]),t._v(" "),s("p",[t._v("完成模型的设计之后，下一步就是通过一定的算法从数据中去估计模型的参数，或推断我们感兴趣的其它未知变量的值")]),t._v(" "),s("p",[t._v("一般而言，确切推断（exact inference）的复杂度很高，于是，人们退而求其次，转而探索具有多项式复杂度的近似推断（approximate inference）方法，主要有三种：")]),t._v(" "),s("ol",[s("li",[t._v("基于平均场逼近 (mean field approximation）的 variational inference（EM 算法就属于这类型算法的一种特例）")]),t._v(" "),s("li",[t._v("Belief propagation（最早作为精确推断技术提出，后来衍生出多种近似推断算法 -- 西瓜书）")]),t._v(" "),s("li",[t._v("蒙特卡罗采样（Monte Carlo sampling)")])]),t._v(" "),s("p",[t._v("（蒙特卡罗方法本身也是现代统计学中一个非常重要的分支）")]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"书"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#书"}},[t._v("#")]),t._v(" 🍉书")]),t._v(" "),s("p",[t._v("概率图模型是一类用图来表达变量相关关系的概率模型，分类同上")]),t._v(" "),s("h3",{attrs:{id:"常见模型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#常见模型"}},[t._v("#")]),t._v(" 常见模型")]),t._v(" "),s("p",[t._v("隐马尔可夫模型（Hidden Markov Model，简称 HMM）是结构最筒单的动态贝叶斯网"),s("br"),t._v("\n马尔可夫随机场（Markov Random Field，简称 MRF）是典型的马尔可夫网"),s("br"),t._v("\n条件随机场（Conditional Random Field，简称 CRF）是一种判别式无向图模型")]),t._v(" "),s("h3",{attrs:{id:"学习与推断"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#学习与推断"}},[t._v("#")]),t._v(" 学习与推断")]),t._v(" "),s("p",[t._v("给定参数 "),s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[s("semantics",[s("mrow",[s("mi",{attrs:{mathvariant:"normal"}},[t._v("Θ")])],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("\\Theta")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"0.6833em"}}),s("span",{staticClass:"mord"},[t._v("Θ")])])])]),t._v(" 求解某个变量 "),s("span",{staticClass:"katex"},[s("span",{staticClass:"katex-mathml"},[s("math",{attrs:{xmlns:"http://www.w3.org/1998/Math/MathML"}},[s("semantics",[s("mrow",[s("mi",[t._v("x")])],1),s("annotation",{attrs:{encoding:"application/x-tex"}},[t._v("x")])],1)],1)],1),s("span",{staticClass:"katex-html",attrs:{"aria-hidden":"true"}},[s("span",{staticClass:"base"},[s("span",{staticClass:"strut",staticStyle:{height:"0.4306em"}}),s("span",{staticClass:"mord mathnormal"},[t._v("x")])])])]),t._v(" 的分布，就变成对联合分布中其他无关变量进行积分的过程，这称为「边际化」（marginalization）")]),t._v(" "),s("p",[t._v("精确推断")]),t._v(" "),s("ol",[s("li",[t._v("变量消去")]),t._v(" "),s("li",[t._v("信念传播")])]),t._v(" "),s("p",[t._v("近似推断")]),t._v(" "),s("ol",[s("li",[t._v("采样法")]),t._v(" "),s("li",[t._v("变分推断")])]),t._v(" "),s("h3",{attrs:{id:"话题模型"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#话题模型"}},[t._v("#")]),t._v(" 话题模型")]),t._v(" "),s("hr"),t._v(" "),s("h2",{attrs:{id:"misc"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#misc"}},[t._v("#")]),t._v(" Misc.")]),t._v(" "),s("h3",{attrs:{id:"em-算法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#em-算法"}},[t._v("#")]),t._v(" EM 算法")]),t._v(" "),s("p",[t._v("在有隐变量（无法观测）的概率模型中，用最大似然或最大后验估计参数的方法")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm#Description",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://en.wikipedia.org/wiki/Expectation–maximization_algorithm#Description"),s("OutboundLink")],1)]),t._v(" "),s("p",[t._v("k-means 就是典型的 EM 算法")]),t._v(" "),s("h2",{attrs:{id:"阅读材料"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#阅读材料"}},[t._v("#")]),t._v(" 阅读材料")]),t._v(" "),s("p",[t._v("https://ermongroup.github.io/cs228-notes/")])])}),[],!1,null,null,null);a.default=i.exports}}]);