# æ¢¯åº¦ä¸‹é™ä¼˜åŒ–ç®—æ³•æ€»ç»“

<link rel="stylesheet" href="/notes/katex@0.11.1.min.css">

::: warning ğŸš§
Under construction...
:::

## èƒŒæ™¯

[å›é¡¾ä¸€ä¸‹](./learning-theory.md)ï¼Œç°å®ä¸­æˆ‘ä»¬æœ‰äº†æ•°æ®é›† $D = \lbrace(x^{(1)},y^{(1)}),\cdots,(x^{(m)},y^{(m)})\rbrace$ï¼Œä¹Ÿå®šä¹‰äº†æŸå¤±å‡½æ•° $\ell\colon\mathcal{Y}\times\mathcal{Y}\to\mathbb{R}$ï¼Œæˆ‘ä»¬æƒ³è¦å¯»æ‰¾ä¸€ä¸ªå‡è®¾å‡½æ•° $h$ï¼Œwhich èƒ½æœ€å°åŒ–ç»éªŒè¯¯å·®ï¼ˆ$h$ çš„æ‰€æœ‰å‚æ•°ç”± $\theta$ è¡¨ç¤ºï¼‰ï¼š

$$ \min_\theta f(\theta;D)\text{,}\qquad\text{where } f(\theta;D)=\frac{1}{m}\sum_{i=1}^m \ell\mathopen{}\left(h(x^{(i)};\theta), y^{(i)}\right)\mathclose{}. $$

è¿™ä¸ªä¼˜åŒ–é—®é¢˜å°±å¯ä»¥ç”¨æ¢¯åº¦ä¸‹é™æ¥è§£å†³

$$ \theta^\prime = \theta - \eta\cdot g $$

å…¶ä¸­ $\eta$ ä¸º==å­¦ä¹ ç‡== (learning rate)ï¼Œ$g = \nabla\widehat{E}(\theta)$ ä¸ºæ¢¯åº¦ï¼ˆä¸‹æ–‡æŠŠç›®æ ‡å‡½æ•°ç®€è®°ä¸º $f$ï¼‰

è¿™é‡Œæˆ‘ä»¬å¼•å…¥æ—¶åˆ» $t$ çš„æ ‡è®°ï¼Œå¹¶ä¸”åˆ—å‡ºä¸€äº›é‡è¦çš„é‡

- $g_t = \nabla f(\theta_t)$ï¼Œ**åŸå§‹æ¢¯åº¦**
- $v_t \coloneqq \theta_{t+1} - \theta_t$ï¼Œ**å®é™…æ›´æ–°é‡**

å¾ˆæ˜¾ç„¶ï¼Œå¯¹äºéšæœºæ¢¯åº¦ä¸‹é™ (SGD) æ¥è¯´ï¼Œ$v_t = \eta\cdot g_t$ã€‚ï¼ˆTODO ä¸€é˜¶äºŒé˜¶åŠ¨é‡ï¼‰

## æœ´ç´ 

### æŒ‘æˆ˜

- é€‰æ‹©å­¦ä¹ ç‡
- é€‰æ‹©å­¦ä¹ ç‡ scheduler
- å¦‚æœæ•°æ®ç¨€ç–æˆ–ç‰¹å¾é¢‘ç‡ä¸åŒï¼ˆï¼Ÿï¼Ÿï¼Ÿï¼‰ï¼Œæˆ‘ä»¬æƒ³ç»™ä¸å¸¸å‡ºç°çš„ç‰¹å¾æ›´å¤§çš„ update
- å±€éƒ¨æå°å€¼

### Momentum

$$
\begin{alignedat}{2}
    v_t &= \gamma\,v_{t-1} + \eta\,\nabla f(\theta_t) \\
    \theta_{t+1} &= \theta_t - v_t.
\end{alignedat}
$$

### Nesterov accelerated gradient

$$
\begin{alignedat}{2}
    v_t &= \gamma\, v_{t-1} + \eta\,\nabla f(\theta_t - \gamma\, v_{t-1}) \\
    \theta_{t+1} &= \theta_t - v_t.
\end{alignedat}
$$

::: tip
PyTorch ä¸­ SGD + Momentum çš„[å®ç°æ–¹å¼](https://pytorch.org/docs/master/optim.html#torch.optim.SGD)ä¸ä¸Šè¿°å…¬å¼ç•¥æœ‰ä¸åŒï¼Œå­¦ä¹ ç‡ $\eta$ æ˜¯ä¹˜åœ¨ $v_t$ ä¸Šè€Œä¸æ˜¯æ¢¯åº¦ $g$ ä¸Š
:::

### Adagrad

$$ g_{t,i} = \nabla f(\theta_{t,i}). $$

($\theta_{t+1,i} = \theta_{t,i} - \eta\cdot g_{t,i}$)

$$ \theta_{t+1,i} = \theta_{t,i} - \frac{\eta}{\textcolor{#FF7800}{\sqrt{G_{t,ii}+\epsilon}}}\cdot g_{t,i} $$

$G_t$

Adadelta

RMSprop

Adam

(AdaMax, Nadam, AMSGrad)

---

## é˜…è¯»ææ–™

- [ä¸€ä¸ªæ¡†æ¶çœ‹æ‡‚ä¼˜åŒ–ç®—æ³•ä¹‹å¼‚åŒ SGD/AdaGrad/Adam - çŸ¥ä¹ä¸“æ ](https://zhuanlan.zhihu.com/p/32230623)
- [An overview of gradient descent optimization algorithms](https://ruder.io/optimizing-gradient-descent/index.html)
- Wilson, Ashia C., et al. **The marginal value of adaptive gradient methods in machine learning**. In *Advances in neural information processing systems*. 2017. [link](https://proceedings.neurips.cc/paper/2017/hash/81b3833e2504647f9d794f7d7b9bf341-Abstract.html)
- https://towardsdatascience.com/adam-latest-trends-in-deep-learning-optimization-6be9a291375c
