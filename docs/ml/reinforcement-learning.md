---
title: å¼ºåŒ–å­¦ä¹ 
---

# å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)

<link rel="stylesheet" href="/notes/katex.min.css">

::: warning
UNDER CONSTRUCTION
:::

::: callout ğŸ¥¥ In a Nutshell
è®¾æƒ³ä½ æ­£åœ¨ä¸‹ä¸€å±€ï¼ˆä½ ï¼‰ä¸çŸ¥é“è§„åˆ™çš„æ£‹ï¼Œä¸‹äº†å‡ åï¼ˆä¸Šç™¾ï¼‰æ­¥ä¹‹åï¼Œè£åˆ¤çªç„¶å®£å¸ƒã€Œä½ è¾“äº†ã€
â”€â”€ï¼ˆå¦‚ä½•ä¸‹å¾—æ›´å¥½ï¼‰è¿™å°±æ˜¯å¼ºåŒ–å­¦ä¹ 
:::

::: warning TODO
å¼ºåŒ–å­¦ä¹ çš„ç‰¹ç‚¹ï¼šä¸çŸ¥é“è§„åˆ™->ï¼Ÿï¼Œå‡ åä¸Šç™¾æ­¥->ï¼Ÿ
:::

==å¼ºåŒ–å­¦ä¹ ==æ˜¯é€šè¿‡ä¸ç¯å¢ƒäº¤äº’æ¥è§£å†³**è¿ç»­å†³ç­–**çš„é—®é¢˜ï¼Œè€Œ**ç›‘ç£å­¦ä¹ **å¯ä»¥çœ‹ä½œæ˜¯**å•è½®å†³ç­–**ï¼ˆé¢„æµ‹ï¼‰é—®é¢˜

- ==å¯èƒ½çš„çŠ¶æ€== **state**ï¼Œ$s \in S$ï¼ˆæ£‹ç›˜çš„å±€é¢ï¼‰
- ==å…è®¸çš„åŠ¨ä½œ== **action**ï¼Œ$a \in A(s)$ï¼ˆå³å¯ä»¥è½å­çš„ä½ç½®ï¼‰
- A ==(probabilistic) transition model==ï¼Œ$P(s^\prime;s, a) \colon S \times A \times S \to [0,1]$ï¼ˆæ¯”å¦‚é»‘ç™½æ£‹çš„ç¿»è½¬ï¼Œå›´æ£‹çš„æå­ï¼‰
- ~~==æ•ˆç”¨å‡½æ•°== **utility function**ï¼Œ$u(s,p)$ï¼Œå³ç©å®¶ $p$ åœ¨æ¸¸æˆç»“æŸæ—¶ï¼ˆçŠ¶æ€ $s$ï¼‰è·å¾—çš„ã€Œæ”¶ç›Šã€~~

## é©¬å°”å¯å¤«å†³ç­–è¿‡ç¨‹ (Markov Decision Processes, aka MDPs)

TODO

## ä» Model-based åˆ° Model-free

## é˜…è¯»ææ–™

- Stuart Russell and Peter Norvig. Artificial Intelligence: A Modern Approach. *Prentice Hall*. 3rd 2009.
  (Chapter 17: Making Complex Decisions; Chapter 21: Reinforcement Learning)
- å‘¨å¿—å. æœºå™¨å­¦ä¹ . 2016 å¹´ç¬¬ 1 ç‰ˆ.ï¼ˆç¬¬ 16 ç« ï¼Œå¼ºåŒ–å­¦ä¹ ï¼‰

å…¶å®ƒ

- [è¿™é‡Œæœ‰ä¸€ç¯‡æ·±åº¦å¼ºåŒ–å­¦ä¹ åŠé€€æ–‡ - çŸ¥ä¹ä¸“æ ](https://zhuanlan.zhihu.com/p/33936457) 2018-02-25
